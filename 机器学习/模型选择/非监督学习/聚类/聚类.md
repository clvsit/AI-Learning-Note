# 聚类
在“无监督学习”（unsupervised learning）中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。

此类学习任务中研究最多、应用最广的是“聚类”（clustering）。**聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）**。通过这样的划分，每个簇可能对应于一些潜在的概念（对应于分类任务中的类别）。

【说明】：这些潜在概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名。

【作用】：
- 作为一个单独过程，用于找寻数据内在的分布结构。
- 作为分类等其他学习任务的前驱过程。例如，在一些商业应用中需对新用户的类型进行判别，但定义“用户类型”对商家来说却可能不太容易，此时往往可先对用户数据进行聚类，根据聚类结构将每个簇定义为一个类，然后再基于这些类训练分类模型，用于判别新用户的类型。

【两个基本问题】：
- 性能度量
- 距离计算

## 性能度量
聚类性能度量亦称聚类“有效性指标”（validity index），与监督学习中的性能度量作用相似。
- 评估聚类结果的好坏；
- 若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。

【问】：什么样的聚类结果比较好呢？

【答】：直观上看，我们希望“物以类聚”，即同一簇的样本尽可能彼此相似，不同簇的样本尽可能不同。换言之，聚类结果的“簇内相似度”（intra-cluster similarity）高且“簇间相似度”（inter-cluster similarity）低。

【分类】：
- 外部指标（external index）：将聚类结果与某个“参考模型”（reference model，例如领域专家给出的划分结果）进行比较；
- 内部指标（internal index）：直接考察聚类结果而不利用任何参考模型。

## 距离计算
距离度量（distance measure）需满足一些基本性质：
- 非负性：`$dist(x_i, x_j) \geq 0$`；
- 同一性：`$dist(x_i, x_j) = 0$` 当且仅当 `$x_i = x_j$`；
- 对称性：`$dist(x_i, x_j) = dist(x_j, x_i)$`；
- 直递性（三角不等式）：`$dist(x_i, x_j) \leq dist(x_i, x_k) + dist(x_k, x_j)$`。

## 原型聚类
原型聚类亦称“基于原型的聚类”（prototype-based clustering），此类算法假设聚类结构能通过一组原型（样本空间中具有代表性的点）刻画，在现实聚类任务中极为常见。

【通常做法】：先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式，将产生不同的算法。

### K-均值算法
给定样本集 `$D = \{x_1, x_2, \cdots, x_m\}$`，K-均值算法针对聚类所得簇划分 `$C = \{C_1, C_2, \cdots, C_k\}$` 最小化平方误差
```math
E = \sum_{i=1}^k\sum_{x \in C_i}||x - \mu_i||_2^2 \quad (1)

\mu_i = \frac{1}{|C_i|}\sum_{x \in C_i}x
```
其中 `$\mu_i$` 是簇 `$C_i$` 的均值向量。例如簇 C1 有三个向量，分别为
```math
x_1 = (1, 2) \quad x_2 = (2, 2) \quad x_3 = (2, 3)

\mu_1 = \frac{1}{3}\sum_{x_i \in C_1}x_i = \frac{1}{3}((1, 2) + (2, 2) + (2, 3)) = (\frac{5}{3}, \frac{7}{3})
```
E 刻画了簇内样本围绕簇均值向量的紧密程度，E 值越小则簇内样本相似度越高。

【难点】：最小化平方误差并不容易，找到最优解需考察样本集 D 所有可能的簇划分，这是一个 NP 难问题。

【方案】：K-均值算法采用了贪心策略，通过迭代优化来近似求解式子（1）。算法流程如下：
- 输入：样本集 `$D = \{x_1, x_2, \cdots, x_n\}$`；聚类簇数 k。
- 输出：簇划分 `$C = \{C_1, C_2, \cdots, C_k\}$`。
- 过程：
1. 从样本集 D 中随机挑选 K 个样本作为初始均值向量；
2. 计算每个样本与各均值向量的距离，并根据距离确定当前样本所属的簇；
3. 依据划分后的簇重新计算均值向量，判断均值向量是否发生变化，若发生变化则更新，否则保持不变；
4. 重复（2）、（3）步骤，直到所有均值向量均未更新。

#### 程序实现
【实现准备】：
- 所需包：numpy

【随机挑选初始均值向量函数】：随机确定起始位置，然后根据样本集长度和挑选数量计算出步长。这么做可以避免挑选出重复值，以及尽可能地从样本集各个区域挑选数据，而不是聚集在某一区域。
```python
def random_select(dataset, count):
    # 获取样本集长度
    length = dataset.shape[0]
    # 计算步长和初始位置
    step, start = length // count, np.random.randint(length)
    data_select = []
    # 按照起始位置和步长挑选数据
    for i in range(count):        
        data_select.append(dataset[start])
        start += step
        start = start % length
    return data_select
```

【实现 K-means 函数】：
- 函数定义

```python
def k_mean(dataset, k):
```
- 获取样本集长度和初始化所需变量

```python
length = dataset.shape[0]
dataset_cluster_info = np.array(np.zeros((length, 2)))
flag = True
```
- flag：用作循环判断的标志；
- dataset\_cluster\_info：创建一个二维数组，行数等于样本集长度，对应样本集数据。列数为二，第一列为当前样本所属的簇，第二列为当前样本距离所属簇的均值向量的值。
- 随机从样本集中挑选 k 个样本向量作为簇的均值向量

```python
cluster_vectors = random_select(dataset, k)
```
- 进入循环，每次清空簇信息并将 flag 变量设置为 False

```python
while flag:
    cluster = []
    flag = False
    # ...
return cluster
```
- 循环每一个样本，将其划分到相应的簇

```python
for i in range(length):
    min_dist = np.inf
    cluster_index = -1
    for index in range(k):
        dist = np.linalg.norm(dataset[i] - cluster_vectors[index])
        if dist < min_dist:
            min_dist = dist
            cluster_index = index
    dataset_cluster_info[i] = (cluster_index, min_dist)
```
- 更新每个簇的均值向量

```python
for i in range(k):
    cluster_data = dataset[dataset_cluster_info[:, 0] == i]
    cluster.append(cluster_data)
    cluster_new = np.mean(cluster_data, axis=0)
    if (cluster_new != cluster_vectors[i]).all():
        flag = True
        cluster_vectors[i] = cluster_new
```

【完整代码】：
```python
def k_mean(dataset, k):
    # 初始化
    length = dataset.shape[0]
    # 随机从样本集中挑选 k 个样本向量作为簇的均值向量
    cluster_vectors = random_select(dataset, k)
    dataset_cluster_info = np.array(np.zeros((length, 2)))
    flag = True
    
    while flag:
        cluster = []
        flag = False
        # 循环每一个样本，将其划分到相应的簇内
        for i in range(length):
            min_dist = np.inf
            cluster_index = -1
            for index in range(k):
                dist = np.linalg.norm(dataset[i] - cluster_vectors[index])
                if dist < min_dist:
                    min_dist = dist
                    cluster_index = index
            dataset_cluster_info[i] = (cluster_index, min_dist)

        # 更新每个簇的均值向量
        for i in range(k):
            cluster_data = dataset[dataset_cluster_info[:, 0] == i]
            cluster.append(cluster_data)
            cluster_new = np.mean(cluster_data, axis=0)
            if (cluster_new != cluster_vectors[i]).all():
                flag = True
                cluster_vectors[i] = cluster_new
    return cluster
```

上述代码的终止条件是当**每个簇的均值向量都不发生变化**。但随着数据量的增加以及数据复杂性的提高，要达到终止条件或许不是一件容易的事情。为避免运行时间过长，通常要设置一个**最大运行轮数**或**最小调整幅度阈值**。若达到最大轮数或调整幅度小于阈值，则停止运行。

我们再对代码进行改进，允许使用者自动选择代码的终止条件。

【改进代码】：
```python
class KMeans:
    
    def __init__(self, k, stop_criterion=True, max_iter=500, min_threshold=1):
        self.k = k
        self.stop_criterion = stop_criterion
        self.max_iter = max_iter
        self.iter = 0
        self.min_threshold = min_threshold
        self.data_cluster_info = None
    
    def _random_select(self, data, k):
        length = data.shape[0]
        step, start = length // k, numpy.random.randint(length)
        data_select = []
        for i in range(k):        
            data_select.append(data[start])
            start += step
            start = start % length
        return data_select
    
    def fit(self, data):
        # 初始化
        length = data.shape[0]
        # 随机从样本集中挑选 k 个样本向量作为簇的均值向量
        cluster_vectors = self._random_select(data, self.k)
        self.data_cluster_info = numpy.array(numpy.zeros((length, 2)))
        flag = True
        self.iter = 0

        while self._stop_criterion():
            cluster = []
            flag = False
            # 循环每一个样本，将其划分到相应的簇内
            for i in range(length):
                min_dist = numpy.inf
                cluster_index = -1
                for index in range(self.k):
                    dist = numpy.linalg.norm(data[i] - cluster_vectors[index], 2)**2
                    if dist < min_dist:
                        min_dist = dist
                        cluster_index = index
                self.data_cluster_info[i] = (cluster_index, min_dist)

            # 更新每个簇的均值向量
            for i in range(self.k):
                cluster_data = data[self.data_cluster_info[:, 0] == i]
                cluster.append(cluster_data)
                cluster_new = numpy.mean(cluster_data, axis=0)
                if (cluster_new != cluster_vectors[i]).all():
                    flag = True
                    cluster_vectors[i] = cluster_new
            
            self.iter += 1
            if not flag:
                break
        return cluster
    
    def _stop_criterion(self):
        # 最大运行轮数
        if self.stop_criterion:
            return True if self.iter <= self.max_iter else False
        # 最小调整幅度阈值
        else:
            if self.iter == 0:
                error = numpy.infty
            else:
                error = numpy.sum(self.data_cluster_info[1], axis=0)
            print(error)
            return True if error > self.min_threshold else False
```

### 学习向量量化
学习向量量化（Learning Vector Quantization，简称 LVQ）与 K 均值算法类似，也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。

给定样本集 `$D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$`，每个样本 `$x_i$` 是由 m 个属性描述的特征向量 `$(x_i^1, x_i^2, \cdots, x_i^m), y_i \in Y$` 是样本 `$x_i$` 的类别标记。

【目标】：学得一组 m 维原型向量 `$\{p_1, p_2, \cdots, p_k\}$`，每个原型向量代表一个聚类簇，簇标记 `$t_j \in Y$`。

【算法描述】：
- 输入：样本集 `$D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$`；原型向量个数 k，各原型向量预设的类别标记 `$\{t_1, t_2, \cdots, t_k\}$`；学习率 `$\eta \in (0, 1)$`。
- 输出：原型向量 `$\{p_1, p_2, \cdots, p_k\}$`。
- 过程：
1. 从样本集 D 中随机选取样本作为原型向量（初始均值向量）；
2. 在每一轮迭代中，随机选取一个样本，并计算该样本与各个原型向量的距离，然后确定簇标记；
3. 根据样本和原型向量的类别标记是否一致来进行相应的更新：
    - 若簇标记相等，则将原型向量向该样本靠近；
    ```math
    p' = p_{j*} + \eta (x_i - p_{j*})
    ```
    - 若簇标记不相等，则将原型向量远离该样本；
    ```math
    p' = p_{j*} - \eta (x_i - p_{j*})
    ```
4. 若满足算法的停止条件，则将当前原型向量作为最终结果返回。

#### 程序实现
【所需包】：
- NumPy

随机挑选原型向量可以直接使用 K-means 编码中已实现的随机挑选函数 random\_select()。

【LVP 函数实现】：
- 随机挑选指定数量的原型向量。

```python
prototypes = random_select(dataset, k)
```
- 开始迭代。

```python
for i in range(n_iters):
    # ...
```
- 迭代过程中，每次随机从样本集中选择一个样本，判断该样本所属的簇。

```python
data = dataset[numpy.random.randint(length)]
    min_dist = numpy.inf
    cluster_index = -1
    for j in range(k):
        dist = numpy.linalg.norm(data[:2] - prototypes[j, :2], 2)**2
        if dist < min_dist:
            min_dist = dist
            cluster_index = j
```
- 接着，判断当前样本的标签类别是否和所属簇的类别标签是否相同。根据不同的情况执行相应的操作。

```python
if data[2] == prototypes[cluster_index, 2]:
    prototypes[cluster_index, :2] += learning_rate * (data[:2] - prototypes[cluster_index, :2])
else:
    prototypes[cluster_index, :2] -= learning_rate * (data[:2] - prototypes[cluster_index, :2])
```
- 迭代结束后，返回原型向量。

```python
return prototypes
```

【完整代码】：
```python
def lvp(dataset, p, n_iters=100, learning_rate=1):
    length = dataset.shape[0]
    prototypes = random_select(dataset, p)
    
    for i in range(n_iters):
        # 随机挑选样本
        data_random = dataset[np.random.randint(length)]
        min_dist = np.inf
        cluster_index = -1
        # 判断当前样本所属的簇
        for j in range(p):
            dist = np.linalg.norm(data_random[:2] - prototypes[j, :2], 2)
            if dist < min_dist:
                min_dist = dist
                cluster_index = j
        if data_random[2] == prototypes[cluster_index, 2]:
            prototypes[cluster_index, :2] += learning_rate * min_dist
        else:
            prototypes[cluster_index, :2] -= learning_rate * min_dist
    return prototypes
```

在学得的一组原型向量 `$\{p_1, p_2, \cdots, p_k\}$` 后，即可实现对样本空间 X 的簇划分。之后，对任意样本 x，将其划入与其距离最近的原型向量所代表的簇中。换言之，每个原型向量 `$p_i$` 定义了与之相关的一个区域 `$R_i$`，该区域中每个样本与 `$p_i$` 的距离不大于它与其他原型向量 `$p_{i'}(i'\neq i)$` 的距离，即
```math
R_i = \{x \in \chi | ||x-p_i||_2 \leq ||x-p_{i'}||_2, i' \neq i \}
```
由此形成了对样本空间 X 的簇划分 `$\{R_1, R_2, \cdots, R_k\}$`，该划分通常称为 “Voronoi 剖分”（Voronoi tessellation）。-

PS：若将 Ri 中样本全用原型向量 pi 表示。则可实现数据的“有损压缩”（lossy compression），这称为“向量量化”（vector quantization）；LVQ 由此而得名。

## 密度聚类
密度聚类亦称“基于密度的聚类”（density-based clustering），此类算法假设聚类结构能通过样本分布的紧密程度确定。

通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩扎安聚类簇以获得最终的聚类结果。

### DBSCAN 算法
DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种著名的密度聚类算法，基于一组“邻域”（neighborhood）参数（`$\epsilon, MinPts$`）来刻画样本分布的紧密程度。

给定数据集 `$D = \{x_1, x_2, \cdots, x_n\}$`，定义如下概念：
- `$\epsilon$`-邻域：对 `$x_i \in D$`，其 `$\epsilon$`-邻域包含样本集 D 中与 `$x_i$` 的距离不大于 `$\epsilon$` 的样本，即
```math
N_{\epsilon}(x_i) = \{x_j \in D | dist(x_i, x_j) \leq \epsilon \}
```
- 核心对象（core object）：若 `$x_i$` 的 `$\epsilon$`-邻域至少包含 MinPts 个样本，即 `$|N_\epsilon(x_i)| \geq MinPts$`，则 `$x_i$` 是一个核心对象；
- 密度直达（directly density-reachable）：若 `$x_i$` 位于 `$x_j$` 的 `$\epsilon$`-邻域中，且 `$x_i$` 核心对象，则称 `$x_i$` 由 `$x_j$` 密度直达。密度直达关系通常不满足对称性；
- 密度可达（density-reachable）：对 `$x_i$` 与 `$x_j$`，若存在样本序列 `$p_1, p_2, \cdots, p_n$`，其中 `$p_1 = x_i, p_n = x_j$` 且 `$p_{i+1}$` 由 `$p_i$` 密度直达，则称 `$x_j$` 由 `$x_i$` 密度可达。密度可达关系满足直递性，但不满足对称性；
- 密度相连（density-connected）：对 `$x_i$` 与 `$x_j$`，若存在 `$x_k$`，使得 `$x_i$` 与 `$x_j$` 均由 `$x_k$` 密度可达，则称 `$x_i$` 与 `$x_j$` 密度相连。密度相连关系满足对称性。
- 噪声点：不属于任何一个簇的样本，从任何一个核心点出发都是密度不可达。
- 边界点：属于某一个类的非核心点，不能发展下线了。

基于这些概念，DBSCAN 将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。形式化地说，给定邻域参数（`$\epsilon$`， MinPts），簇 `$C \subseteq D$` 是满足以下性质的非空样本子集：
- 连接性（connectivity）：`$x_i \in C, x_j \in C \rightarrow x_i \text{与} x_j \text{密度相连}$`；
- 最大性（maximality）：`$x_i \in C, x_j \text{由} x_i \text{密度可达} \rightarrow x_j \in C$`

【问】：如何从数据集 D 中找出满足以上性质的聚类簇呢？

【答】：实际上，若 x 为核心对象，由  x 密度可达的所有样本组成的集合记为 `$X = \{x' \in D | x' \text{由} x \text{密度可达} \}$`，则不难证明 X 即为满足连接性与最大性的簇。

【工作流程】：
1. DBSCAN 算法先根据给定的邻域参数找出所有核心对象；
2. 然后从核心对象集中随机选择一个核心对象作为“种子”（seed）；
3. 以该核心对象为出发点，找出由其密度可达的样本生成聚类簇，直到所有核心对象均被访问过为止。


【参数选择】：
- 半径 r：根据 K 距离来设定：找突变点。
    - K 距离：给定数据集，计算数据集中的每个点到集合 D 的子集 S 中所有点之间的距离，距离按照从小到大的顺序排序，d(k) 被称为 k-距离。
- MinPts：K 距离中 K 的值，一般取的小一些，多次尝试。

【优势】：
- 不需要指定簇个数
- 可以发现任意形状的簇
- 擅长找到离群点（检测任务）
- 两个参数就够了

【劣势】：
- 高维数据有些困难（可以做降维）
- 参数难以选择（参数对结果的影响非常大）
- Sklearn 中效率很慢（数据削减策略）

## 层次聚类
层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。

【数据集的划分策略】：
- “自底向上”的聚合策略；
- “自顶向下”的分拆策略。

### AGNES
AGNES（AGglomerative NESting 的简写）是一种采用自底向上聚合策略的层次聚类算法。

【工作过程】：
1. 先将数据集中的每个样本看作一个初始聚类簇；
2. 然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并；
3. 步骤（2）不断重复，直至达到预设的聚类簇的个数。

【关键】：如何计算聚类簇之间的距离。

实际上，每个簇是一个样本集合，因此，只需采用关于集合的某种距离即可。
```math
\text{最小距离：}d_{min}(C_i, C_j) = min_{x\in C_i,z\in C_j}dist(x,z),

\text{最大距离：}d_{max}(C_i, C_j) = max_{x\in C_i,z\in C_j}dist(x,z),

\text{平均距离：}d_{avg}(C_i, C_j) = \frac{1}{|C_i||C_j|}\sum_{x\in C_i}\sum_{z\in C_j}dist(x,z).
```
显然，最小距离由两个簇的最近样本决定，最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本共同决定。

当聚类簇距离为 
- `$d_{min}$`：AGNES 算法被称为“单链接”（single-linkage）；
- `$d_{max}$`：AGNES 算法被称为“全链接”（complete-linkage）；
- `$d_{avg}$`：AGNES 算法被称为“均链接”（average-linkage）。

【算法描述】：
- 输入：样本集 `$D = \{x_1, x_2, \cdots, x_n\}$`;聚类簇距离度量函数 dist；聚类簇数 k。
- 输出：簇划分 `$C = \{C_1, C_2, \cdots, C_k\}$`。
- 过程：
1. 为每个样本创建一个簇；
2. 计算距离矩阵；
3. 开始合并簇过程，初始化聚类簇个数 q = n：
    1. 每次从距离矩阵中找出距离最近的两个聚类簇 `$C_i$` 和 `$C_j$`，i < j；
    2. 合并这两个簇（优先合并到下标较小的簇 `$C_i$`）`$C_i = C_i \bigcup C_j$`；
    3. 将聚类簇重新编号（合并到下标较小的簇可以减少重编号的次数）；
    4. 删除距离矩阵的第 j 行与第 j 列；
    5. 计算合并后的簇 `$C_i$` 与剩余其他簇之间的距离，并更新距离矩阵。
    6. q = q - 1。
    7. 直到 q == k 时，退出循环。
4. 返回簇划分。
    
#### 程序实现
【距离计算函数】：
```python
def get_dist(XA, XB, type='min'):
    if len(XA.shape) == 1:
        XA = np.array([XA])
    if len(XB.shape) == 1:
        XB = np.array([XB])
    dist = 0
    if type == 'min':
        dist = cdist(XA, XB, 'euclidean').min()
    elif type == 'max':
        dist = cdist(XA, XB, 'euclidean').max()
    else:
        dist = cdist(XA, XB, 'euclidean').sum() / XA.shape[0] / XB.shape[0]
    return dist
```

【AGNES 函数】：
- 函数定义

```python
def AGNES(dataset, k, dist_method='avg'):
```

- 初始化所需变量

```python
# 获取样本集长度
length = dataset.shape[0]
# 初始化聚类簇和距离矩阵
clusters, dist_matrix = [], np.mat(np.zeros((length, length)))
```
- 为每个样本分配一个聚类簇

```python
for data in dataset:
    clusters.append(data)
```
- 计算距离矩阵

```python
for i in range(length):
    for j in range(length):
        if i == j:
            dist = np.inf
        else:
            dist = get_dist(clusters[i], clusters[j], dist_method)
        dist_matrix[i, j] = dist
        dist_matrix[j, i] = dist
```
- 设置当前聚类簇的个数，并开始合并过程

```python
cluster_count = length
while cluster_count > k:
```
- 找出距离最近的两个聚类簇

```python
first, second = np.where(dist_matrix == dist_matrix.min())[0]
```
- 合并这两个聚类簇

```python
clusters[first] = np.vstack((cluters[first], clusters[second]))
```
- 将聚类簇重新编号

```python
for i in range(second + 1, cluster_count):
    clusters[i - 1] = clusters[i]
clusters.pop()
```
- 删除距离矩阵的第 second 行与列

```python
dist_matrix = np.delete(dist_matrix, second, axis=0)
dist_matrix = np.delete(dist_matrix, second, axis=1)
```
- 重新计算距离矩阵第 first 簇与其他簇之间距离

```python
for i in range(cluster_count - 1):
    if first == i:
        dist = np.inf
    else:
        dist = get_dist(clusters[first], clusters[i], dist_method)
    dist_matrix[first, i] = dist
    dist_matrix[i, first] = dist
```
- q = q - 1

```python
cluster_count -= 1
```
- 返回簇划分

```python
return clusters
```

【完整代码】：
```python
def AGNES(dataset, k, dist_method='avg'):
    length = dataset.shape[0]
    clusters = []
    dist_matrix = np.mat(np.zeros((length, length)))
    for data in dataset:
        clusters.append(data)
    for i in range(length):
        for j in range(length):
            if i == j:
                dist = np.inf
            else:
                dist = get_dist(clusters[i], clusters[j], dist_method)
            dist_matrix[i, j] = dist
            dist_matrix[j, i] = dist

    # 设置当前聚类簇的个数
    cluster_count = length
    
    while cluster_count > k:
        # 找出距离最近的两个聚类簇
        first, second = np.where(dist_matrix == dist_matrix.min())[0]
        
        # 合并这两个聚类簇
        clusters[first] = np.vstack((clusters[first], clusters[second]))
        
        # 重新编号聚类簇
        for i in range(second + 1, cluster_count):
            clusters[i - 1] = clusters[i]
        clusters.pop()
        
        # 删除距离矩阵的第 second 行与列
        dist_matrix = np.delete(dist_matrix, second, axis=0)
        dist_matrix = np.delete(dist_matrix, second, axis=1)        
        
        # 重新计算距离矩阵第 first 簇与其他簇之间距离
        for i in range(cluster_count - 1):
            if first == i:
                dist = np.inf
            else:
                dist = get_dist(clusters[first], clusters[i], dist_method)
            dist_matrix[first, i] = dist
            dist_matrix[i, first] = dist
        cluster_count -= 1
    return clusters
```
【其他】：豪斯多夫距离（Hausdorff distance）可用于集合间的距离计算。